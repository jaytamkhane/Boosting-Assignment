# Boosting Techniques ‚Äì Assignment

This repository contains theoretical and practical questions related to various **Boosting techniques** in Machine Learning. The assignment covers fundamental concepts and implementation of models like AdaBoost, Gradient Boosting, XGBoost, and CatBoost.

---

## Theoretical Questions

Explore essential questions such as:

- What is Boosting in Machine Learning?
- Differences between Boosting and Bagging
- Working principles of AdaBoost
- Understanding Gradient Boosting and its loss function
- Enhancements introduced by XGBoost
- CatBoost and its advantages for categorical data
- Real-world applications of Boosting
- Key hyperparameters and regularization in boosting models
- Feature Importance in boosting algorithms

---

## Practical Tasks

Hands-on implementation includes:

- Training AdaBoost, Gradient Boosting, XGBoost, and CatBoost on real datasets
- Evaluating performance using metrics such as Accuracy, MAE, R¬≤, MSE, F1-Score, Log-Loss
- Visualizing Feature Importance, Learning Curves, Confusion Matrix, and ROC Curve
- Performing hyperparameter tuning using GridSearchCV
- Working with imbalanced datasets and adjusting class weights
- Comparing the effects of different learning rates and estimators

---

## üõ† Technologies Used

- Python
- Scikit-learn
- XGBoost
- CatBoost
- Matplotlib / Seaborn (for visualization)
- Jupyter Notebook

---

## üìÅ Structure

- [Boosting_Assignment.ipynb](https://github.com/jaytamkhane/Boosting-Assignment/blob/main/Boosting_Assignment.ipynb) # Solutions With Questions
- [README.md] # Readme file 
